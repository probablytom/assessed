@article{Kitchenham2004,
  author = {Kitchenham, Barbara},
  title = {{Procedures for Performing Systematic Reviews}},
  year = {2004}
}

@article{Kitchenham2013,
abstract = {Context: Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research. Objective: To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process. Method: We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools. Results: We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult. Conclusion: We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Kitchenham, Barbara and Brereton, Pearl},
journal = {Information and Software Technology},
number = {12},
pages = {2049--2075},
publisher = {Elsevier B.V.},
title = {{A systematic review of systematic review process research in software engineering}},
volume = {55},
year = {2013}
}

@book{khan2001undertaking,
  title={Undertaking systematic reviews of research on effectiveness: CRD's guidance for carrying out or commissioning reviews},
  author={Khan, Khalid S and Ter Riet, Gerben and Glanville, Julie and Sowden, Amanda J and Kleijnen, Jos and others},
  number={4 (2n)},
  year={2001},
  publisher={NHS Centre for Reviews and Dissemination}
}

@article{Kampenes2007,
  abstract = {An effect size quantifies the effects of an experimental treatment. Conclusions drawn from hypothesis testing results might be erroneous if effect sizes are not judged in addition to statistical significance. This paper reports a systematic review of 92 controlled experiments published in 12 major software engineering journals and conference proceedings in the decade 1993-2002. The review investigates the practice of effect size reporting, summarizes standardized effect sizes detected in the experiments, discusses the results and gives advice for improvements. Standardized and/or unstandardized effect sizes were reported in 29{\%} of the experiments. Interpretations of the effect sizes in terms of practical importance were not discussed beyond references to standard conventions. The standardized effect sizes computed from the reviewed experiments were equal to observations in psychology studies and slightly larger than standard conventions in behavioral science. ?? 2007 Elsevier B.V. All rights reserved.},
  author = {Kampenes, Vigdis By and Dyb{\aa}, Tore and Hannay, Jo E. and Sj{\o}berg, Dag I K},
  journal = {Information and Software Technology},
  title = {{A systematic review of effect size in software engineering experiments}},
  year = {2007}
}

@article{Sjoberg2005,
  abstract = {The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.},
  author = {Sj{\o}berg, Dag I K and Hannay, Jo E. and Hansen, Ove and Kampenes, Vigdis By and Karahasanovi{\'{c}}, Amela and Liborg, Nils Kristian and Rekdal, Anette C.},
  journal = {IEEE Transactions on Software Engineering},
  number = {9},
  pages = {733--753},
  title = {{A survey of controlled experiments in software engineering}},
  volume = {31},
  year = {2005}
}

@article{Smite2010,
abstract = {Recognized as one of the trends of the 21st century, globalization of the world economies brought significant changes to nearly all industries, and in particular it includes software development. Many companies started global software engineering (GSE) to benefit from cheaper, faster and better development of software systems, products and services. However, empirical studies indicate that achieving these benefits is not an easy task. Here, we report our findings from investigating empirical evidence in GSE-related research literature. By conducting a systematic review we observe that the GSE field is still immature. The amount of empirical studies is relatively small. The majority of the studies represent problem-oriented reports focusing on different aspects of GSE management rather than in-depth analysis of solutions for example in terms of useful practices or techniques. Companies are still driven by cost reduction strategies, and at the same time, the most frequently discussed recommendations indicate a necessity of investments in travelling and socialization. Thus, at the same time as development goes global there is an ambition to minimize geographical, temporal and cultural separation. These are normally integral parts of cross-border collaboration. In summary, the systematic review results in several descriptive classifications of the papers on empirical studies in GSE and also reports on some best practices identified from literature.},
author = {{\v{S}}mite, Darja and Wohlin, Claes and Gorschek, Tony and Feldt, Robert},
journal = {Empir Software Eng},
pages = {91--118},
title = {{Empirical evidence in global software engineering: a systematic review}},
volume = {15},
year = {2010}
}

@techreport{Kitchenham2007,
author = {Kitchenham, Barbara and Charters, Stuart},
title = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
year = {2007}
}

@article{Benavides,
  abstract = {Software product line engineering is about producing a set of related products that share more commonalities than variabilities. Feature models are widely used for variability and commonality management in software product lines. Feature models are information models where a set of products are represented as a set of features in a single model. The automated analysis of feature models deals with the computerâ€“aided extraction of information from feature models. The literature on this topic has contributed with a set of operations, techniques, tools and empirical results which have not been surveyed until now. This paper provides a comprehensive literature review on the automated analysis of feature models 20 years after of their invention. This paper contributes by bringing together previously-disparate streams of work to help shed light on this thriving area. We also present a conceptual framework to understand the different proposals as well as categorise future contributions. We finally discuss the different studies and propose some challenges to be faced in the future.},
  author = {Benavides, David and Segura, Sergio and Ruiz-Cort{\'{e}}s, Antonio},
  title = {{Automated Analysis of Feature Models 20 Years Later: A Literature Review}},
  year = {2010},
  journal = {Information Systems}
}

@article{Webster2002,
  author = {Webster, Jane and Watson, Richard T.},
  journal = {MIS Quarterly},
  number = {2},
  title = {{Analysing the Past to Prepare for the Future: Writing a Literature Review}},
  volume = {26},
  year = {2002}
}

@article{Beecham2007,
abstract = {This research presents a systematic literature review of motivation in Software Engineering. The objective is to report on what motivates and de-motivates developers, and how existing models address motivation. The majority of studies find Software Engineers form a distinguishable occupational group. Results indicate that Software Engineers are likely to be motivated according to: their 'characteristics' (e.g., their need for variety); internal 'controls' (e.g., their personality) and external 'moderators' (e.g., their career stage). Models of motivation in Software Engineering are disparate and do not reflect the complex needs of Software Engineers in their different career stages, cultural and environmental settings.},
author = {Beecham, Sarah and Baddoo, Nathan and Hall, Tracy and Robinson, Hugh and Sharp, Helen},
title = {{Motivation in Software Engineering: A Systematic Literature Review}},
year = {2007}
}

