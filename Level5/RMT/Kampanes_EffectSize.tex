\subsection{Effect Size in Software Engineering}
% \subsubsection{Systematic Method}
First reviewed is a study into effect size in software engineering experiments:~\citet*{Kampenes2007}. The review was conducted according to systematic review guidelines --- the guidelines were not specified, however the procedure roughly aligned with that of Kitchenham, and Kitchenham's guidelines were cited as reasoning when defining a search criteria.\par

As required when developing a systematic review,~\cite{Kampenes2007} used search criteria to determine the literature which was to be reviewed. This search criteria was cited from another paper (\cite{Sjoberg2005}, which shares some authors), rather than stated directly; the collated work was a set of software engineering papers exhibiting controlled experiments published over the span of 10 years.\par

The method for data extraction was also well reported, as a systematic review should entail by Kitchenham's guidelines. The paper goes on to perform a deep and thorough statistical analysis, and concludes with a review of the results of these analyses with a comparison to the results of similar papers in Psychology and Behavioural Science. The paper succeeded in selecting several papers with empirical data so as to perform the statistical analysis with a large sample.\par

The authors found 78 usable studies for their review, from the 5453 studies assessed.\par

% \subsubsection{Data Extracted}
% This paper confirmed Kitchenham's doubting note in this case. While a large sample was indeed selected after a time --- 92 papers over the course of 10 years --- selecting those papers required reviewing the contents of 5453 software engineering papers for suitable results. In the comparison with other fields, the similar Behavioural Science paper found 475 papers with suitable data to review. A similar education literature review found 226, published within a span of a single year.\par
% 
% Of those 5453 experiments --- 1.4\% of the original sample --- only 78 articles actually contained the controlled experiments sought by the authors. When compared to the similar work in other fields, computing science papers were thrice as likely to report effect size as education papers. However, education papers with suitable empirical experiments were published almost thirty times more frequently. Controlled experiments are therefore significantly less readily available than in other subjects where systematic reviews are a suitable method of literature review.\par
% 
% This does not dictate that systematic reviews should not be carried out in software engineering --- it does suggest, though, that empirical data might not be very generally available. It would therefore require researchers to wait a large span of time for suitable quantities of data to be produced to create a systematic review.\par
% 
