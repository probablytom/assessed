\include{preamble}
\begin{document}
\maketitle

\begin{abstract}
Systematic reviewing is a technique for bringing scientific rigour to a computer science literature review, pioneered by Barbara Kitchenham~\citep{Kitchenham2004}. 12 years after Kitchenham's original guidelines were set for structuring a systematic literature review, the technique has seen widespread adoption --- but the original guidelines raise questions and note possible issues with the method. A review of these systematic reviews may highlight whether these concerns are worth revisiting, before Kitchenham's guidelines and those like them become standard practice for the software engineering research community.
\end{abstract}

\section{Introduction}
% Explore what a systematic review is, and why we're exploring them as a topic.
A Systematic Review is a literature review which collates the results of many papers, using statistical analysis to draw empirical results about the state of a research field and to answer research questions posited as the motivation for the review. Systematic reviews are born from the philosophy that a literature review should have scientific merit, and produce reproducible results. This technique somewhat opposes standard techniques for literature reviews, which leave more room for subjective insight. The scientific nature of a systematic literature review, in theory, removes ambiguity and bias from literature review practice and lends the review the same validity and credence as a research study, though a statistical analysis of collated results.\par

% Introduce the notion that there's a problem with the technique --- bring up the original guidelines.
Kitchenham's systematic literature review technique\footnote{Kitchenham's guidelines have undergone some revisions over time --- the two most cited versions in the papers reviewed being~\cite{Kitchenham2004} and~\cite{Kitchenham2007}. As the latter is an incremental improvement over the former, all of Kitchenham's guideline versions will be referred to through this document simply as Kitchenham's guidelines.} has begun to dominate as a literature review technique for software engineering. Conventional computing science literature reviews might closer resemble Webster's guidelines~\citep{Webster2002}, which are less rigorous, and less focused on empiricism and repeatability, yet offer structure to the review. However, doubts about systematic reviews are sometimes raised. For example, in Kitchenham's own guidelines:

\begin{displayquote}
    In particular, software engineering research has relatively little empirical research compared with the large quantities of research available on medical issues, and research methods used by software engineers are not as rigorous as those used by medical researchers.
\end{displayquote}~\cite{Kitchenham2004}

The quality of this data in real-world systematic reviews can only be apparent once systematic reviewing has become mainstream, which it now has. Therefore, in this review, a series of systematic literature reviews will be analysed and searched for their scrutiny of research rigour and format of empirical data. In this way, the importance of this doubt regarding the suitability of systematic reviews for software engineering research will be assessed. We will see that there is some reason for this doubt, and offer solutions to the problem as it manifests.\par

The reviews chosen were picked as a result of their popularity on the ``\emph{Google Scholar}'' academic search engine, found by a search for ``software engineering ``systematic'' literature review'', and similar searches. This was to find papers which were well-cited and high-impact, because as the question to be answered would impact the culture around systematic reviews, these papers are important, as they are most likely to influence future systematic reviews.\par


\section{Papers reviewed}
\input{Kampanes_EffectSize}
\input{Smite_GSE}
\input{Benavides_Automated}
\input{Beecham_Motivation}
\input{Chen_Variability}
\input{Kitchenham_Systematic}


\section{Discussion}\label{sec:discussion}
% Worth pointing out that you can't subject a regular review to the same sort of scrutiny as a systematic review
% - At least you're guaranteed data and rigour as a reader
% - Maybe this is something semantic guidelines might be able to provide?

% Best thing we can say about all of these papers is that regardless of whether they use statistics, they're repeatable. 
% - Does repeatability matter in this case?
% - There may well be useful insight in the motivation case, for example, but simple counts of papers were presented instead. 
% - Results seem cursory this way --- never much food for thought, and a literature review is the best opportunity for thought food: someone's just read loads of papers and can tell us things we don't know about the field at large!

The systematic reviews selected have all been reviews of literature pertaining to a niche in software engineering. Interestingly, a common theme emerged, which was that there was frequently limited quantitative data for the reviewers to analyse statistically.\par

This seemed to occur for two reasons:
\begin{enumerate}
    \item Some subject areas did not easily produce empirical quantitative data. An example of this would be the~\cite{Benavides} article on Motivation; achieving consistently reliable quantitative data in psychological or ethnographic problem domains is difficult to do.
    \item The research culture of some subject areas did not seem inclined to promote the production and publication of quantitative data as a part of their work. Examples of this would be~\cite{Chen2007}, or~\cite{Smite2010}. 
\end{enumerate}

Unsurprisingly,~\cite{Kitchenham2013} succeeds in creating statistical analyses from the taxonomic data the authors collate over the course of the review. Also worth noting is that another paper,~\cite{Kampenes2007}, \emph{did} succeed in creating statistical measurements from the taxonomical data produced. One of the authors, however --- Dyba --- has their own systematic review procedure published~\citep{dybaguidelines}, so this comes at little surprise.\par

For systematic reviews to have a more scientific advantage over more orthodox review techniques, quantitative data evaluation is paramount. It remains the most reliable and clearest metric of a piece of scientific research's reproducibility.\par

For review culture where textual analysis is intended to go hand-in-hand with repeatability, a review method such as~\cite{Webster2002} may be more appropriate. Webster's method offers a helpful guide for getting away from the ``phonebook'' issues some literature reviews encounter. In addition, the repeatable nature of the taxonomical data they produce fits nicely in line with Webster's method.\par

An alternative method for solving these issues would be to adopt the statistical analysis methods from~\cite{Kitchenham2013} or~\cite{Chen2007}. This would allow at least modest statistical analysis of taxonomical data, should researchers be particularly inclined toward the systematic philosophy of literature reviewing.\par

Also worth noting is the high number of papers often needed to search to collect a sufficient sample to analyse from the search criteria Kitchenham advocates. Specifically:

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllll@{}}
\toprule
                & \cite{Kampenes2007} & \cite{Smite2010} & \cite{Chen2007} & \cite{Benavides} & \cite{Kitchenham2013} & \cite{Beecham2007} \\ \midrule
Papers Found    &    5453      &  387                    &   628   &    72       &    410        &  519       \\
Papers Reviewed &     78     &    59                     &   34   &     53      &     45       &     92    \\
\% Utilised     &     1.43     &   15.24                  &  5.5    &   73.61        &  10.98          &   17.73      \\ \bottomrule
\end{tabular}%
}
\caption{Papers reviewed to achieve desired samples}
\end{table}
\section{Conclusion}

% Did the literature reviews matter?
The literature reviewed here, while of quality and all of impact, did not consistently fulfil the criteria which provides value to a systematic review. Indeed, Kitchenham's early doubt --- that software engineering might not produce enough quantitative data to bring scientific rigour to the review in this case --- seems well-placed in this instance.\par

Many solutions are available to remedy this, however. Guidelines more in line with statistical analysis of taxonomical data is one option --- of which~\cite{Kitchenham2013} and~\cite{Kampenes2007} are good examples. Other options would be the adoption of structured review methods where systematic reviews may be less appropriate, for which~\cite{Webster2002} would be a good choice. A final option would be, as a culture, to shift more toward quantitative data production and analysis\dots{} though this seems impractical as a solution.\par

% Things to note when writing this:
% - Do the literature reviews having ``systematic'' pedigree make them more susceptible to draw conclusions which aren't actually important? Can they get away with saying unimportant things, because the systematic review makes them sound more important?
% - Perhaps a systematic review should be something chosen only if a review turns out to produce lots of data to analyse? Should they necessarily start with rigour? If yes, how can we ensure more data of higher quality?


\bibliographystyle{plainnat}
\bibliography{biblio}

\end{document}
